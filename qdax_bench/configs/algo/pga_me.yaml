defaults:
  - default_qd # Inherit all settings from default_qd
  - _self_ # Allow overriding inherited values

config_origin: QDax_bench

plotting:
  algo_name: PGA-ME

# Factory
factory: 
  _target_: qdax_bench.factories.pga_me.PGAMEFactory

emitter: 
  _target_: qdax.core.emitters.pga_me_emitter.PGAMEEmitter
  _partial_: true

  variation_fn: 
    _target_: qdax.core.emitters.mutation_operators.isoline_variation
    _partial_: true
    line_sigma: ${algo.params.line_sigma}
    iso_sigma: ${algo.params.iso_sigma}

  config:
    _target_: qdax.core.emitters.pga_me_emitter.PGAMEConfig
    # _partial_: true
    env_batch_size: ${algo.params.batch_size}
    proportion_mutation_ga: 0.5
    num_critic_training_steps: ${algo.params.num_critic_training_steps}
    num_pg_training_steps: ${algo.params.num_pg_training_steps}

    # TD3 params
    replay_buffer_size: 1_000_000
    critic_hidden_layer_size: ${algo.params.critic_hidden_layer_size}
    critic_learning_rate: 3e-4
    greedy_learning_rate: 3e-4
    policy_learning_rate: 5e-3
    noise_clip: 0.5
    policy_noise: 0.2
    discount: 0.99
    reward_scaling: 1.0
    batch_size: 100
    soft_tau_update: 0.005
    policy_delay: 2


group_defaults:
  kheperax:
    iso_sigma: 0.2
    line_sigma: 0.0
    batch_size: 64  
    critic_hidden_layer_size: [16]
    num_pg_training_steps: 10
    num_critic_training_steps: 300
  brax:
    iso_sigma: 0.005
    line_sigma: 0.05
    batch_size: 256
    critic_hidden_layer_size: [256, 256]
    num_pg_training_steps: 150
    num_critic_training_steps: 3000
